{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "弹性分布式数据集（Resient Distributed Dataset,RDD）,不可变的分布式对象集合\n",
    "<br> 1 每个RDD都被分为多个分区，这些分区运行在不同节点上。\n",
    "<br> 2 RDD可以包含Python,Java,Scala中任意类型的对象，甚至是用户自定义的对象。\n",
    "<br>  Spark中所有对数据的操作：\n",
    "<br> 1 创建RDD\n",
    "<br> 2 转化已有RDD,定义新的RDD\n",
    "<br> 3 对需要重用的中间结果RDD执行persist()操作\n",
    "<br> 4 调用RDD行动操作进行一次并行计算求值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建RDD\n",
    "    <br> 1 读取外部数据集\n",
    "    <br> 2 驱动器程序中对一个对象集合进行并行化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#引入并初始化pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#python中初始化spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"my app\")#集群的URL，应用名\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建一个名为README.MD的RDD，弹性分布式数据集\n",
    "lines = sc.textFile(\"README.MD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建出来的RDD支持两种类型的操作：\n",
    "<br> 转化操作(transformation)，由一个RDD生成一个新的RDD，比如map(),filter()\n",
    "<br> 行动操作(action)，对RDD计算出一个结果，会触发实际计算，比如count(),first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#转化操作(transformation)\n",
    "dayLines = lines.filter(lambda line: \"day\" in line)#filter传入的是一个函数\n",
    "dayLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The baby eagle liked the nest. It was the only world he had ever known. It was warm and comfortable, had a great view, and even better, he had all the food and love and attention that a great mother eagle could provide. Many times each day the mother would swoop2 down from the sky and land in the nest and feed the baby eagle delicious morsels3 of food. She was like a god to him, he had no idea where she came from or how she worked her magic.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 行动操作(action)\n",
    "dayLines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark是惰性计算RDD，只有第一次行动操作中到时，才会真正计算（节省存储空间）\n",
    "<br>  默认情况下，Spark的RDD会在每次进行进行操作时重新计算，如果想要多个行动操作重用同一个RDD，则可以使用RDD.persist()将该RDD缓存下来（磁盘或内存中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "The baby eagle liked the nest. It was the only world he had ever known. It was warm and comfortable, had a great view, and even better, he had all the food and love and attention that a great mother eagle could provide. Many times each day the mother would swoop2 down from the sky and land in the nest and feed the baby eagle delicious morsels3 of food. She was like a god to him, he had no idea where she came from or how she worked her magic.\n"
     ]
    }
   ],
   "source": [
    "#把RDD持久化到内存中\n",
    "dayLines.persist()\n",
    "print dayLines.count()\n",
    "print dayLines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[7] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SparkContext.parallelize()方法创建RDD，只在开发原型和测试时使用，需要将数据都放在一台机器内存中\n",
    "lines = sc.parallelize([\"pandas\",\"i like pandas\"])\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baby eagle liked the nest. It was the only world he had ever known. It was warm and comfortable, had a great view, and even better, he had all the food and love and attention that a great mother eagle could provide. Many times each day the mother would swoop2 down from the sky and land in the nest and feed the baby eagle delicious morsels3 of food. She was like a god to him, he had no idea where she came from or how she worked her magic.\n",
      "Until one day, the mother stopped coming to the nest.\n",
      "Then one day the mother eagle appeared at the top of the mountain cliff, with a big bowl of delicious food and she looked down at her baby. The baby looked up at the mother and cried \"Why did you abandon me? I'm going to die any minute. How could you do this to me?\"\n",
      "A few days later, \"I'm going to end it all,\" he said. \"I give up. It is time for me to die.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'The baby eagle liked the nest. It was the only world he had ever known. It was warm and comfortable, had a great view, and even better, he had all the food and love and attention that a great mother eagle could provide. Many times each day the mother would swoop2 down from the sky and land in the nest and feed the baby eagle delicious morsels3 of food. She was like a god to him, he had no idea where she came from or how she worked her magic.',\n",
       " u'Until one day, the mother stopped coming to the nest.',\n",
       " u'Then one day the mother eagle appeared at the top of the mountain cliff, with a big bowl of delicious food and she looked down at her baby. The baby looked up at the mother and cried \"Why did you abandon me? I\\'m going to die any minute. How could you do this to me?\"',\n",
       " u'A few days later, \"I\\'m going to end it all,\" he said. \"I give up. It is time for me to die.\"']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dayline in dayLines.take(5):#RDD.take(5)只返回5个元素\n",
    "    print dayline\n",
    "#RDD.collect()返回RDD所有元素\n",
    "dayLines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4向Spark传递函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines: ['pandas', 'i like pandas']\n",
      "filterLines1: ['pandas', 'i like pandas']\n",
      "filterLines2: ['pandas', 'i like pandas']\n"
     ]
    }
   ],
   "source": [
    "print \"lines:\",lines.collect()\n",
    "filterLines1 = lines.filter(lambda x: \"pandas\" in x)\n",
    "print \"filterLines1:\",filterLines1.collect()\n",
    "def containPandas(x):\n",
    "    return \"pandas\" in x\n",
    "filterLines2 = lines.filter(containPandas)\n",
    "print \"filterLines2:\",filterLines2.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5常见转化操作和行动操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1不限定RDD类型的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1 (1)元素级转化操作\n",
    "<br> map 将函数应用到RDD中的每个元素，将返回值构成新的RDD\n",
    "<br> filter 将函数应用到RDD中的每个元素，将返回迭代器中的所有内容构成新的RDD\n",
    "<br> flatMap 将函数应用于RDD的每个元素，通过filter函数的返回元素组成RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[1, 4, 9, 16]\n",
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "rddTest = sc.parallelize([1,2,3,4])\n",
    "print rddTest.collect()\n",
    "rddTestMap = rddTest.map(lambda x: x**2)\n",
    "print rddTestMap.collect()\n",
    "rddTestFilter = rddTest.filter(lambda x: x>2)\n",
    "print rddTestFilter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'yeah']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap()输入的每一个元素，输出多个元素，常用于字符串分隔\n",
    "rddTest2 = sc.parallelize([\"hello world\",\"yeah\"])\n",
    "rddTest2.collect()\n",
    "rddFlatFlatMap = rddTest2.flatMap(lambda line:line.split())\n",
    "rddFlatFlatMap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1 (2)伪集合操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> distinct去重\n",
    "<br> union并集\n",
    "<br> intersection交集\n",
    "<br> subtract减\n",
    "<br> cartesian笛卡尔积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddTest5: [1, 2, 3]\n",
      "[1, 2, 3, 3, 3, 4, 5]\n",
      "[3]\n",
      "[2, 1]\n",
      "[(1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 3), (3, 4), (3, 5), (3, 3), (3, 4), (3, 5)]\n"
     ]
    }
   ],
   "source": [
    "rddTest3 = sc.parallelize([1,2,3,3])\n",
    "rddTest4 = sc.parallelize([3,4,5])\n",
    "rddTest5 = rddTest3.distinct()\n",
    "print \"rddTest5:\",rddTest5.collect()\n",
    "print rddTest3.union(rddTest4).collect()\n",
    "print rddTest3.intersection(rddTest4).collect()\n",
    "print rddTest3.subtract(rddTest4).collect()\n",
    "print rddTest3.cartesian(rddTest4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1 (3)行动操作\n",
    "<br> collect() 返回RDD中所有元素\n",
    "<br> count() RDD中元素的个数\n",
    "<br> countByValue() RDD中各元素出现的次数\n",
    "<br> take(num) 从RDD中返回num个元素\n",
    "<br> top(num) 从RDD中返回最前面的num个元素\n",
    "<br> takeSample(num) 从RDD中返回任意一些元素\n",
    "<br> reduce() 并行整合RDD中所有元素，类似于sum()\n",
    "<br> fold()与reduce()类似，区别在于每个分区第一次调用时加上个初始值，作为结果\n",
    "<br> foreach()，对RDD中所有元素使用给定的函数，但是无返回值；将数据存到数据库，发送数据等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddTest6 = sc.parallelize([1,2,3,3])\n",
    "print rddTest6.collect()\n",
    "print rddTest6.count()\n",
    "print rddTest6.countByValue()\n",
    "print rddTest6.take(2)\n",
    "print rddTest6.top(2)\n",
    "print rddTest6.takeSample(False,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func1 = lambda x: x+1\n",
    "rddTest6.foreach(func1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# reduce()接收一个函数作为参数，此函数操作两个相同元素类型的RDD数据，并返回一个同样类型的新元素\n",
    "sum = rddTest6.reduce(lambda x,y: x + y)\n",
    "print sum\n",
    "#fold()与reduce()类似，区别在于每个分区第一次调用时加上个初始值，作为结果\n",
    "sum2 = rddTest6.fold(1,lambda x,y: x + y)\n",
    "print sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (3, 1), (3, 1)]\n",
      "(9, 4)\n",
      "average: 2.25\n"
     ]
    }
   ],
   "source": [
    "#map,reduce组合方式求列表平均值\n",
    "rddTest7 = rddTest6.map(lambda x:(x,1))\n",
    "print rddTest7.collect()\n",
    "turple_result = rddTest7.reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "print turple_result\n",
    "from __future__ import division\n",
    "print \"average:\",turple_result[0]/turple_result[1]\n",
    "# 参考：https://stackoverflow.com/questions/51236850/how-to-find-an-average-for-a-spark-rdd\n",
    "# val pair = sc.parallelize(1 to 100)\n",
    "# .map(x => (x, 1))\n",
    "# .reduce((x, y) => (x._1 + y._1, x._2 + y._2))\n",
    "# val mean = pair._1 / pair._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average: 2.25\n"
     ]
    }
   ],
   "source": [
    "#aggregate()函数计算列表平均值\n",
    "#aggragate函数解释参考：https://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark\n",
    "#函数原型：aggregate(zeroValue, seqOp, combOp)\n",
    "#函数功能：aggregate() lets you take an RDD and generate a single value that is of a different type than what was stored in the original RDD.\n",
    "#seqOp是每个partitions的操作\n",
    "#例子：Compute the sum of a list and the length of that list. Return the result in a pair of (sum, length)\n",
    "seqOp = (lambda local_result, list_element: (local_result[0] + list_element, local_result[1] + 1) )\n",
    "#comOp将各partitions的内容聚合\n",
    "combOp = (lambda some_local_result, another_local_result: (some_local_result[0] + another_local_result[0], some_local_result[1] + another_local_result[1]) )\n",
    "tuple_result2 = rddTest6.aggregate((0,0),seqOp,combOp)\n",
    "print \"average:\",tuple_result2[0]/tuple_result2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.2不同类型RDD相互转化\n",
    "<br> python所有的函数都实现在基本的RDD类中，但是如果操作对应的RDD数据类型不正确，就会导致运行时错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6 持久化缓存\n",
    "<br> 1 Spark RDD 是惰性求值的，每次行动操作都会重算RDD及所有依赖，为了避免重复运算同一个RDD，可以将数据持久化。\n",
    "<br> 2 Spark有不同的持久化级别，只内存；内存+磁盘；只磁盘等\n",
    "<br> 3 unpersist()方法可以手动把持久化的数据从缓存中删除。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
