{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pair RDD键值对RDD，通常用来进行聚合操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.2 创建PairRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#引入并初始化pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#python中初始化spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"my app\")#集群的URL，应用名\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建一个名为README.MD的RDD，弹性分布式数据集\n",
    "lines = sc.textFile(\"README.MD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Once',\n",
       "  u'Once upon a time there was a baby eagle living in a nest perched on a cliff overlooking a beautiful valley with waterfalls and streams, trees and lots of little animals, scurrying1 about enjoying their lives.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairLines = lines.map(lambda x:(x.split()[0],x))\n",
    "pairLines.take(1)#.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.3 Pair RDD转化操作\n",
    "    按功能区分：\n",
    "    聚合操作\n",
    "    数据分组\n",
    "    连接\n",
    "    数据排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.3.1 聚合操作\n",
    "    reduceByKey()函数，合并具有相同键的值\n",
    "    combineBykey()函数，使用不同的返回类型合并具有相同键的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('panda', 0), ('pink', 3), ('pirate', 3), ('panda', 3), ('pink', 4)]\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([(\"panda\",0),(\"pink\",3),(\"pirate\",3),(\"panda\",3),(\"pink\",4)])\n",
    "print pairRDD.collect()\n",
    "print type(pairRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('panda', (0, 1)), ('pink', (3, 1)), ('pirate', (3, 1)), ('panda', (3, 1)), ('pink', (4, 1))]\n",
      "[('pink', (7, 2)), ('panda', (3, 2)), ('pirate', (3, 1))]\n",
      "pairRDDReduceByKey2: [('pink', (3, 1, 4, 1)), ('panda', (0, 1, 3, 1)), ('pirate', (3, 1))]\n",
      "每个键对应的平均值 [3.5, 1.5, 3.0]\n"
     ]
    }
   ],
   "source": [
    "#计算每个键对应的平均值\n",
    "pairRDDmapValues = pairRDD.mapValues(lambda x:(x,1))#mapValues()函数，对pairRDD中每个值应用一个函数而不改变键\n",
    "print pairRDDmapValues.collect()\n",
    "pairRDDReduceByKey = pairRDDmapValues.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))#reduceByKey()函数，合并具有相同键的值\n",
    "print pairRDDReduceByKey.collect()\n",
    "\n",
    "from __future__ import division\n",
    "pairRDDAverageByKey = pairRDDReduceByKey.map(lambda x: x[1][0]/x[1][1])\n",
    "print \"每个键对应的平均值\",pairRDDAverageByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pink', 3.5), ('panda', 1.5), ('pirate', 3.0)]\n",
      "{'pink': 3.5, 'panda': 1.5, 'pirate': 3.0}\n"
     ]
    }
   ],
   "source": [
    "#(sum,count)\n",
    "# 参考：https://github.com/mahmoudparsian/pyspark-tutorial/blob/master/tutorial/combine-by-key/spark-combineByKey.md\n",
    "sumCount = pairRDD.combineByKey(lambda value: (value, 1),#createCombiner 参数对应PairRDD的value，\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),#mergeValue参数一个combiner （sum,count）和 一个new value\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])#mergeCombiner,(sum, count)\n",
    "                           )\n",
    "\n",
    "averageByKey = sumCount.map(lambda (key, (totalSum, count)): (key, totalSum / count))\n",
    "print averageByKey.collect()\n",
    "print averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#(sum,count)\n",
    "#参考：https://github.com/mahmoudparsian/pyspark-tutorial/blob/master/tutorial/combine-by-key/spark-combineByKey.md\n",
    "sumCount = pairRDD.combineByKey(lambda value: (value, 1),#createCombiner 参数对应PairRDD的value，\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),#mergeValue参数一个combiner （sum,count）和 一个new value\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])#mergeCombiner,(sum, count)\n",
    "                           )\n",
    "\n",
    "averageByKey = sumCount.map(lambda (key, (totalSum, count)): (key, totalSum / count))\n",
    "print averageByKey.collect()\n",
    "print averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {u'all': 5, u'looked': 4, u'cried': 4, u'better,': 1, u'top': 1, u'\"Why': 1, u'liked': 1, u'fair!\"': 1, u'comfortable,': 1, u'down.': 1, u'view,': 1, u'lives.': 1, u'Until': 1, u'ground': 1, u'up.': 1, u'time,': 1, u'His': 1, u'cliff,': 1, u'just': 1, u'mother': 10, u'had': 4, u'him,': 1, u'him': 1, u'to': 11, u'only': 1, u'going': 2, u'mother.': 1, u'enjoying': 1, u'morsels3': 1, u'anger.': 1, u'faster.': 2, u'do': 3, u'good': 1, u'\"Come': 1, u'Faster': 1, u'get': 2, u'food': 3, u'Many': 1, u'trees': 1, u'It': 3, u'craved4.': 1, u'She': 2, u'mother,\"': 1, u'world': 1, u\"I'm\": 2, u'hungry.': 1, u'day': 2, u'Once': 1, u'and': 23, u'How': 1, u'like': 1, u'did': 1, u'die': 1, u'Over': 1, u'nourishing': 1, u'meal,\"': 1, u'soon,': 2, u'stopped': 1, u'she': 6, u'always': 1, u'where': 1, u'vision': 1, u'few': 1, u'\"How?\"': 1, u'nest.': 3, u'\"You\\'re': 1, u'some': 1, u'idea': 1, u'up': 4, u'perched': 1, u'picked': 1, u'die.\"': 1, u'whined': 2, u'me?': 1, u'out': 1, u'even': 1, u'living': 1, u'what': 1, u'said': 2, u'swoop2': 1, u'for': 1, u'lots': 1, u'god': 1, u'face.': 1, u'bad': 1, u'him.': 1, u'time.': 2, u'coming,\"': 1, u'ever': 1, u'He': 8, u'eagle': 9, u'attention': 2, u'each': 1, u'animals,': 1, u'death': 2, u'come': 2, u'\"Here': 1, u'cried.': 2, u'great': 2, u'ate': 1, u'last': 2, u'her': 2, u'of': 5, u'could': 4, u'provide.': 1, u'terrible': 1, u'\"I\\'m': 3, u'times': 1, u\"isn't\": 1, u'or': 1, u'feed': 1, u'beautiful': 1, u'nearby.': 1, u'own': 1, u'love': 2, u'appeared': 1, u'sky': 1, u'it.\"': 1, u'one': 3, u'down': 4, u'die,\"': 1, u'delicious': 2, u'your': 1, u'minute.': 1, u'little': 1, u'from': 2, u'would': 2, u'rushed': 1, u'\"Eat': 1, u\"it's\": 1, u'all,\"': 1, u'there': 3, u'tears': 1, u'But': 1, u'their': 1, u'much': 1, u'time': 3, u'valley': 1, u'was': 8, u'baby,': 1, u'baby.': 1, u'first.': 1, u'on': 1, u\"didn't\": 1, u'magic.': 1, u'about': 2, u'but': 2, u'you': 4, u'cried,': 2, u'hear': 1, u'was.': 1, u'baby': 10, u'closer,': 1, u'worked': 1, u'\"This': 1, u'with': 6, u'he': 10, u'me': 1, u'land': 1, u'grew': 2, u'eagle,': 1, u'streaming': 1, u'this': 2, u'clearly,': 1, u'this,': 1, u'fell.': 1, u'screamed.': 2, u'abandon': 1, u'mountain': 1, u'\"Very': 1, u'known.': 1, u'meal.': 1, u'over.': 1, u'his': 5, u'give': 1, u'is': 4, u'later,': 1, u'it': 1, u'days': 1, u'pushed': 1, u'visualize7': 1, u'tasty': 1, u'at': 4, u'have': 1, u'in': 3, u'bowl': 1, u'said,': 1, u'any': 1, u'said.': 3, u'away.': 1, u'Head': 1, u'end': 1, u'dying': 1, u'no': 2, u'scurrying1': 1, u'that': 1, u'whined6': 1, u'\"I': 1, u'me?\"': 2, u'how': 1, u'big': 1, u'\"How': 1, u'streams,': 1, u'A': 1, u'it!\"': 1, u'sure': 1, u'Then': 2, u'nest': 3, u'felt': 1, u'upon': 1, u'food,': 1, u'food.': 1, u'speed.': 2, u'flew': 1, u'coming': 2, u'The': 10, u'waterfalls': 1, u'cliff': 1, u'a': 11, u'swooped5': 1, u'faster': 1, u'overlooking': 1, u'dying,\"': 1, u'hungry': 1, u'complained.': 1, u'more': 1, u'warm': 1, u'so': 2, u'came': 1, u'Picked': 1, u'day,': 1, u'very': 2, u'the': 22, u'sharp.': 1, u'strong.': 1, u'know': 1})\n"
     ]
    }
   ],
   "source": [
    "#单词计数\n",
    "lines.collect()\n",
    "words = lines.flatMap(lambda x:x.split())\n",
    "words.collect()\n",
    "wordsMap = words.map(lambda x:(x,1))#单词为键，1构成的pair RDD\n",
    "wordsMap.collect()\n",
    "wordsCount = wordsMap.reduceByKey(lambda x,y:x+y)\n",
    "wordsCount.collect()\n",
    "\n",
    "#countByvalue()单词计数\n",
    "countbyValue = words.countByValue()\n",
    "type(countbyValue)\n",
    "print countbyValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    并行度调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 4), ('b', 4)]\n",
      "[('b', 4), ('a', 4)]\n"
     ]
    }
   ],
   "source": [
    "#reduceByKey()\n",
    "data = sc.parallelize([(\"a\",3),(\"b\",4),(\"a\",1)])\n",
    "print data.reduceByKey(lambda x,y:x+y).collect()\n",
    "print data.reduceByKey(lambda x,y:x+y,10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    4.3.2数据分组\n",
    "    groupByKey根据pairRDD的键对数据进行分组，RDD(K,V)->RDD(K,Iterable(V))\n",
    "    cogroup()函数对多个共享同一个键的RDD进行分组，例如RDD(K,V),RDD(K,W)>RDD(K,(Iterable(V),Iterable(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x6d3a6a0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x6d3a710>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([(\"a\",3),(\"b\",4),(\"a\",1)])\n",
    "data.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.3.3连接    PairRDD之间用键连接起来，常用连接方式左外连接，右外连接，内连接，交叉连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (3, 2)), ('b', (4, 5))]\n",
      "[('a', (3, 2)), ('c', (1, None)), ('b', (4, 5))]\n",
      "[('a', (3, 2)), ('b', (4, 5)), ('d', (None, 6))]\n"
     ]
    }
   ],
   "source": [
    "data1 = sc.parallelize([(\"a\",3),(\"b\",4),(\"c\",1)])\n",
    "data2 = sc.parallelize([(\"a\",2),(\"b\",5),(\"d\",6)])\n",
    "print data1.join(data2).collect()#内连接\n",
    "print data1.leftOuterJoin(data2).collect()#左外连接\n",
    "print data1.rightOuterJoin(data2).collect()#右外连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.3.4数据排序\n",
    "    RDD.sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda> at 0x7fc35dbcf848)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('whose', 6), ('white', 9), ('was', 8), ('Mary', 1), ('little', 4), ('lamb', 5), ('had', 2), ('fleece', 7), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print sc.parallelize(tmp).sortByKey().collect()\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "print sc.parallelize(tmp2).sortByKey(False, 3, keyfunc=lambda k: k.upper()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.4PairRDD行动操作\n",
    "    所有基础RDD的行动操作都可应用于PairRDD，另外PairRDD也有专属的行动操作\n",
    "    countByKey()每个键对应的元素计数，返回字典\n",
    "    collectAsMap()将结果以字典的形式返回，以便查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {1: 1, 3: 2})\n",
      "{1: 2, 3: 6}\n",
      "2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "result1 = sc.parallelize([(1,2),(3,4),(3,6)]).countByKey()#defaultdict(int, {1: 1, 3: 2})\n",
    "result2 = sc.parallelize([(1,2),(3,4),(3,6)]).collectAsMap()\n",
    "print result1\n",
    "print result2\n",
    "print result2[1]\n",
    "print result2[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.5数据分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
